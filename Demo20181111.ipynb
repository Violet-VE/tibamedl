{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "img = Image.open('bw.jpg').convert('L')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'> 0 255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(type(img), np.min(img), np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(397, 600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array(img) \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29, 31, 33, ..., 20, 20, 20],\n",
       "       [30, 31, 33, ..., 20, 20, 20],\n",
       "       [32, 32, 34, ..., 20, 20, 20],\n",
       "       ...,\n",
       "       [34, 33, 33, ..., 81, 80, 78],\n",
       "       [29, 29, 28, ..., 77, 74, 71],\n",
       "       [27, 27, 27, ..., 73, 69, 66]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "img = Image.open('color.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.JpegImagePlugin.JpegImageFile'> 0 255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(type(img), np.min(img), np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822, 600, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array(img) \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IM2COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 7, 7)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x1 = np.random.rand(1,3,7,7)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 75)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1 = im2col(x1, 5, 5,stride=1, pad = 0)\n",
    "col1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用ANN 辨識手寫數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3_2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes  = 10 \n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_input,)))\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.2395 - acc: 0.9291 - val_loss: 0.1064 - val_acc: 0.9690\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0920 - acc: 0.9722 - val_loss: 0.0890 - val_acc: 0.9720\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0602 - acc: 0.9815 - val_loss: 0.0708 - val_acc: 0.9764\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0441 - acc: 0.9860 - val_loss: 0.0709 - val_acc: 0.9790\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 44us/step - loss: 0.0327 - acc: 0.9893 - val_loss: 0.0778 - val_acc: 0.9774\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0243 - acc: 0.9920 - val_loss: 0.0618 - val_acc: 0.9809\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0217 - acc: 0.9927 - val_loss: 0.0802 - val_acc: 0.9767\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0163 - acc: 0.9945 - val_loss: 0.0847 - val_acc: 0.9783\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0184 - acc: 0.9937 - val_loss: 0.0720 - val_acc: 0.9811\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0922 - val_acc: 0.9760\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.0121 - acc: 0.9958 - val_loss: 0.0842 - val_acc: 0.9796\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 47us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.0888 - val_acc: 0.9776\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0866 - val_acc: 0.9796\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 46us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0829 - val_acc: 0.9812\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0885 - val_acc: 0.9814\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用CNN 辨識手寫數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 28,28,1)\n",
    "x_test = x_test.reshape(10000, 28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定網路參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_classes  = 10 \n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立卷積神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size = (3,3),\n",
    "            padding = 'same',\n",
    "            input_shape = (28,28,1), \n",
    "            activation = 'relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(n_hidden_1, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.2094 - acc: 0.9400 - val_loss: 0.0830 - val_acc: 0.9740\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0676 - acc: 0.9797 - val_loss: 0.0604 - val_acc: 0.9820\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 22s 369us/step - loss: 0.0430 - acc: 0.9869 - val_loss: 0.0446 - val_acc: 0.9855\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0310 - acc: 0.9910 - val_loss: 0.0529 - val_acc: 0.9823\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 22s 364us/step - loss: 0.0227 - acc: 0.9932 - val_loss: 0.0462 - val_acc: 0.9847\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 22s 362us/step - loss: 0.0165 - acc: 0.9950 - val_loss: 0.0463 - val_acc: 0.9866\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 22s 363us/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.0407 - val_acc: 0.9874\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 22s 361us/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.0422 - val_acc: 0.9863\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 21s 349us/step - loss: 0.0074 - acc: 0.9977 - val_loss: 0.0516 - val_acc: 0.9847\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 21s 356us/step - loss: 0.0062 - acc: 0.9982 - val_loss: 0.0530 - val_acc: 0.9858\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.0067 - acc: 0.9979 - val_loss: 0.0441 - val_acc: 0.9882\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 22s 364us/step - loss: 0.0046 - acc: 0.9986 - val_loss: 0.0512 - val_acc: 0.9870\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 22s 364us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0599 - val_acc: 0.9848\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 22s 366us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.0536 - val_acc: 0.9875\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 22s 371us/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0513 - val_acc: 0.9885\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('OCR.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 16)\n",
      "(16,)\n",
      "(3136, 256)\n",
      "(256,)\n",
      "(256, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "for ele in model.get_weights():\n",
    "    print(ele.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3136"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14 * 14 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 805,802\n",
      "Trainable params: 805,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model2 = load_model('OCR.hd5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy\n",
    "img  = Image.open('4.png').convert('L')\n",
    "img  = numpy.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = (255 - img) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 64 into shape (28,28,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-db2edf2d3ae2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 64 into shape (28,28,1)"
     ]
    }
   ],
   "source": [
    "img2.reshape(28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
