{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/08/25e47a53692c2e0dcd2211a493ddfe9007a5cd92e175d6dffa6169a0b392/tensorflow-1.14.0-cp37-cp37m-win_amd64.whl (68.3MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.4)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/32/e7/478737fd426798caad32a2abb7cc63ddb4c12908d9e03471dd3c41992b05/grpcio-1.23.0-cp37-cp37m-win_amd64.whl (1.6MB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Collecting absl-py>=0.7.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/46/8b/5e77963dac4a944a0c6b198c004fac4c85d7adc54221c288fc6ca9078072/protobuf-3.9.1-cp37-cp37m-win_amd64.whl (1.0MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.4)\n",
      "Building wheels for collected packages: gast, absl-py, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast absl-py termcolor\n",
      "Installing collected packages: google-pasta, astor, keras-applications, grpcio, gast, absl-py, protobuf, termcolor, markdown, tensorboard, tensorflow-estimator, keras-preprocessing, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 grpcio-1.23.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.1.1 protobuf-3.9.1 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow 分類MNIST 數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0817 09:46:32.937018  5792 deprecation.py:323] From <ipython-input-4-91ef6a97d5b7>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0817 09:46:32.938018  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0817 09:46:32.939018  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0817 09:46:33.404045  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 09:46:33.677061  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0817 09:46:33.679060  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 09:46:33.882072  5792 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot\n",
    "- [小資女, 貴婦, 學生, 小資女, 貴婦] \n",
    "- [1,2,0,1,2]\n",
    "\n",
    "        ``` \n",
    "        [\n",
    "        [0,1,0],\n",
    "        [0,0,1],\n",
    "        [1,0,0],\n",
    "        [0,1,0],\n",
    "        [0,0,1],\n",
    "\n",
    "        ]\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢視數據"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x000000000A8A1BE0>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000000115853C8>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000000124B8860>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.argmax(mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist.train.images[0].reshape((392,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = mnist.train.images[0].reshape((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1411ecc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定網路參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "n_samples  = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 784 (input) => hidden1(256) => hidden2(256) => output(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義Input & Output (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input]) # 784 pixels\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) # 10 digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建構神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):  \n",
    "    # First Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) # W * X + B\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定權重與偏倚(Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), # 784 * 256\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), # 256 * 256\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))# 256 * 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])), # 256\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])), # 256\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))  # 10 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定代價函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 784 (input) => hidden1(256) => hidden2(256) => output(10) => SoftMax => Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0817 10:05:25.782813  5792 deprecation.py:323] From <ipython-input-33-c98ca870291b>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行會話"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=162.63954968539153\n",
      "Epoch: 2 cost=58.55973821640019\n",
      "Epoch: 3 cost=42.77415552486071\n",
      "Epoch: 4 cost=34.342986064390715\n",
      "Epoch: 5 cost=29.041468033357106\n",
      "Epoch: 6 cost=25.374405524947438\n",
      "Epoch: 7 cost=22.484230911081493\n",
      "Epoch: 8 cost=20.2814972386577\n",
      "Epoch: 9 cost=18.46488879030401\n",
      "Epoch: 10 cost=16.913518527773288\n",
      "Epoch: 11 cost=15.684016455303532\n",
      "Epoch: 12 cost=14.505820028375492\n",
      "Epoch: 13 cost=13.619367648999802\n",
      "Epoch: 14 cost=12.714606029770584\n",
      "Epoch: 15 cost=11.960446925921866\n",
      "Training Completed in 15 Epochs\n"
     ]
    }
   ],
   "source": [
    "# Start the session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Intialize all the variables\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Initial the cost = 0.0\n",
    "    avg_cost = 0.0\n",
    "\n",
    "    total_batch = int(n_samples/batch_size) # 55000 / 100 = 550\n",
    "\n",
    "    for i in range(total_batch): # 550\n",
    "\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size) # 100\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        # Compute average loss\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print(\"Epoch: {} cost={}\".format(epoch+1,avg_cost))\n",
    "\n",
    "print(\"Training Completed in {} Epochs\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9136\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用TensorBoard 視覺化呈現網路建構過程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 讀取MNIST數據集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定網路參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "n_samples  = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定義Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    x = tf.placeholder(\"float\", [None, n_input],   name= 'input_x')\n",
    "    y = tf.placeholder(\"float\", [None, n_classes], name= 'input_y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 產生影像檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input_reshape'):\n",
    "    image_input = tf.reshape(x,[-1,28,28,1])\n",
    "    tf.summary.image('input', image_input, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建構多層神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(x, input_tensors, output_tensors, layer_name, activation_function = None):  \n",
    "    with tf.name_scope('Layer'):\n",
    "        with tf.name_scope('Weights'):\n",
    "            weight = tf.Variable(tf.random_normal([input_tensors, output_tensors]), name = 'w')\n",
    "            tf.summary.histogram(name = layer_name + '/Weights', values = weight)\n",
    "        with tf.name_scope('Bias'):\n",
    "            bias = tf.Variable(tf.random_normal([output_tensors]), name= 'b')\n",
    "            tf.summary.histogram(name = layer_name + '/Bias', values = bias)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            formula = tf.add(tf.matmul(x, weight), bias)\n",
    "            \n",
    "        if activation_function is None:\n",
    "            outputs = formula\n",
    "        else:\n",
    "            outputs = activation_function(formula)\n",
    "            \n",
    "        tf.summary.histogram(name = layer_name + '/Outputs', values = outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 添加隱藏層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = add_layer(x, input_tensors = n_input, output_tensors = n_hidden_1, layer_name='layer1',activation_function = tf.nn.relu)\n",
    "layer2 = add_layer(layer1, input_tensors = n_hidden_1, output_tensors = n_hidden_2, layer_name='layer2',activation_function = tf.nn.relu)\n",
    "out_layer = add_layer(layer2, input_tensors = n_hidden_2, output_tensors = n_classes, layer_name='out_layer',activation_function = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定代價函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=y))\n",
    "    tf.summary.scalar('loss', cost)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(out_layer, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化變量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=158.2789852697198\n",
      "Epoch: 2 cost=61.26732438694344\n",
      "Epoch: 3 cost=45.19476784359327\n",
      "Epoch: 4 cost=36.654014079353985\n",
      "Epoch: 5 cost=31.002138905525197\n",
      "Epoch: 6 cost=27.05141910119492\n",
      "Epoch: 7 cost=24.055611841678633\n",
      "Epoch: 8 cost=21.74014560786156\n",
      "Epoch: 9 cost=19.77620756062594\n",
      "Epoch: 10 cost=18.164622361985124\n",
      "Epoch: 11 cost=16.743623175390738\n",
      "Epoch: 12 cost=15.541510716568329\n",
      "Epoch: 13 cost=14.447600691318511\n",
      "Epoch: 14 cost=13.61334546051242\n",
      "Epoch: 15 cost=12.724130941900336\n",
      "Training Completed in 15 Epochs\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    ## Merge Summary\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"tensorboard3/\", graph = sess.graph)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(n_samples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c, result = sess.run([optimizer, cost, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            ## Adding summary of each step\n",
    "            writer.add_summary(result,  epoch * total_batch + i)\n",
    "\n",
    "        print(\"Epoch: {} cost={}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Training Completed in {} Epochs\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打開tensorboard\n",
    "\n",
    "- 打開 anaconda prompt\n",
    "- tensorboard --logdir tensorboard3/ --host 127.0.0.1\n",
    "- http://127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
